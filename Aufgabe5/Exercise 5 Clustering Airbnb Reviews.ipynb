{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb Reviews Dataset\n",
    "\n",
    "The dataset you'll be working with in this exercise is a sample of Airbnb Review data. Airbnb provides all kinds of data concerning quarterly or monthly data of residences in big cities all around the world. The data can be downloaded for free at <a href=\"http://insideairbnb.com/get-the-data/\">inside airbnb</a>.\n",
    "\n",
    "The part of the data relevant to this exercise, as mentioned above, relates to reviews. Because of airbnb's generousity the review datasets for the different cities contain millions of entries which might be a little too much for your first attempt at analyzing text only.\n",
    "\n",
    "For this reason a random subset of 25,000 entries has been prepared for you. In comparison to the original dataset this might seem like a small dataset but some of the following steps to analyze this data might take a couple of seconds or even minutes to compute, depending on your hardware.\n",
    "\n",
    "The dataset in `sample_reviews.csv` is a subset of reviews of airbnbs in the city of Berlin. It also has been prefiltered to only contain reviews which could be identified as English by <a href=\"https://pypi.org/project/langdetect/\"> Nakatani Shuyo's language detection library</a>.\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "- Import necessary package to read `sample_reviews.csv`\n",
    "- Read `sample_reviews.csv` and create a `pandas.DataFrame` with the review data. Note that it has an integer index in its first column.\n",
    "- Inspect the data using the DataFrame's functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary package\n",
    "import pandas as pd\n",
    "\n",
    "#Read sample of review data\n",
    "df = ___________\n",
    "\n",
    "#Inspect the data by using the DataFrame's functionality\n",
    "print(__________)\n",
    "print(__________)\n",
    "print(__________)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorizing Text Data\n",
    "\n",
    "As you might have guessed already the only column of the `sample_reviews` data is the `comments` column which contains the text of a given review. As the machine learning models you got to know over the time of the semester don't handle blobs of text as only input very well some kind of further transformation is needed.\n",
    "\n",
    "One of the less complex ways to represent blobs of text in a way a machine can understand is to utilize CountVectorization. This technique seperates the different words of a text from each other and counts their occurence in the given text. At this point it is important to know in which language the text you will be transforming is written. If you don't know the language it can be detected  by language detectors like the one mentioned above. In this case the language recognition and filtering has already been done for you. All reviews in the data were detected as English.\n",
    "\n",
    "The next step after figuring out the language is to delete stop words from the texts. This is a very important step in almost all analysis of natural language. Stop words are the words that are most frequently used in a language and therefore might not be really carrying a lot of information to analyse. Some examples of stop words in the English language are: `the`, `is`, `which` or `on`. These words are helpful for humans to communicate information fluently and more understandable but in most cases do not carry enough information to be of high importance for a machine learning algorithmn.\n",
    "\n",
    "The goal of this analysis is clustering the airbnb reviews, so what's needed is the core information of a review which does not include stop words. Another argument to delte stop words is that because of their frequency they might not be a good indicator to seperate the data anyway.\n",
    "\n",
    "One last important concept is the `max_features` attribute of the `CountVectorizer`. It limits the number of dimensions of the transformed dataset to a specified amount and only lists the most frequent words in a text aside from the stopwords. This is important when transforming text data because there is really no theoretical upper bound to how many different words occur in a text. This can lead to enormous numbers of features in the transformed dataset which then increases the risk of facing the negative impact of the curse of dimensionality.\n",
    "\n",
    "### Exercise\n",
    "- Import `CountVectorizer` from scikit-learn's `feature_extraction.text` module.\n",
    "- Initialize the `CountVectorizer` with `english` stop words and a maximum number of `100` features.\n",
    "- Fit the `CountVectorizer` to the data and transform in the same line using `fit_transform()`. Pass only the `comments` column of the DataFrame containing the reviews.\n",
    "- Save the feature names in variable `words` by calling the `CountVectorizer`'s `get_feature_names_out()` function.\n",
    "- Create a `pd.DataFrame` for the transformed features using the output of the `CountVectorizer` and calling its `.toarray()` function as data. Pass the feature names variable to the `columns` argument.\n",
    "- Print the first few rows of the new DataFrame to see if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import CountVectorizer\n",
    "____________________\n",
    "\n",
    "#Initialize CountVectorizer\n",
    "count_vect = _______________(stop_words=________,max_features=______)\n",
    "\n",
    "#Apply CountVectorizer\n",
    "data_count = _____________.fit_transform(__________)\n",
    "\n",
    "#Create DataFrame\n",
    "words = ______________\n",
    "data = pd.DataFrame(__________.toarray(),_____________)\n",
    "\n",
    "#Print first few rows of vectorized review data\n",
    "____________\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "Although we already set the `CountVectorizer`'s `max_feature` argument to just the hundred most frequent words in the corpus of reviews, hundred dimensions are a lot of dimensions and will probably increase the computation time of any given clustering algorithmn because distance needs to be calculated in every dimension.\n",
    "\n",
    "A way of dealing with this is reducing the dimensionality of a dataset but preserving as much information as possible. Probably one of the oldest methods (invented in 1901 by Karl Pearson) that is still used in data science is called `Principal Components Analysis (PCA)`. A key part of its longevity is its strong theoretical foundation and its efficency in terms of processing time. It transforms metrically scaled data into a set of principal components. These resulting principal components are sorted by the amount of variance they explain in the original data. Then only the n components of these are selected that explain the most or a desired ratio (e.g. 95 percent or 90 percent) of the variance of the original dataset.\n",
    "\n",
    "As you might have noticed the PCA interprets the information in a dataset as its variance and therefore aims to represent as much of the variance of the original dataset as possible in the reduced dataset. One of the downsides of this technique is that it only operates on the level of correlations in the dataset which means only linear relationships in the original data can be modelled.\n",
    "\n",
    "The computational efficiency is due to the calculation of principal components being a singular value decomposition which is a fairly simple matrix multiplication of the original data's variance-covariance matrix and the original data matrix itself. Those types of computations can be implemented really efficiently using `numpy` and it's under the hood `C` calculations.\n",
    "\n",
    "An important step before utilizing the `Principal Components Analysis` is scaling the data by using e.g. `sklearn`'s standard scaler.\n",
    "\n",
    "### Exercise\n",
    "- Import `PCA` from `sklearn`'s `decomposition` module.\n",
    "- Initialize a `PCA` object. You want it to output the `10` components that explain the most variance in the original data. Also set the `random_state` to `123` for reproducibility.\n",
    "- Fit the `PCA` object to the count vectorized data.\n",
    "- Transform the count vectorized data into the ten principal components that explain the most of its variance and save the result to a new `pd.DataFrame`.\n",
    "- Print the array of explained variance ratio and its sum using the `PCA` objects `explained_variance_ratio_` attribute.\n",
    "- Print the first few rows of the reduced dataset to see if it worked.\n",
    "- Can explain why you could use the `PCA` in this dataset although it represents text, which is not metrically scaled?\n",
    "- Why didn't you need to use the `StandardScaler` before utilizing the `PCA`'s `.fit()` and `.transform()` methods on the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import PCA\n",
    "_____________________\n",
    "\n",
    "#Initialize PCA object for 10 Principal Components\n",
    "pca = _____(n_components=_____,____________)\n",
    "\n",
    "#Fit PCA object to vectorized review data\n",
    "pca.fit(data)\n",
    "data_reduced = __________(______.transform(data))\n",
    "\n",
    "#Print amount of explainend variance by first 10 principal components\n",
    "print(_______________________)\n",
    "print(_______________________.sum())\n",
    "\n",
    "#Print first few rows of reduced data\n",
    "_______________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empty Markdown for your answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "As already mentioned the target of this analysis is clustering the sample of Airbnb reviews in the city of Berlin and interpreting the results. Clustering is, as well as most algorithmns for dimensionality reduction, part of the machine learning discipline of `unsupervised learning`. That means that the dataset does not contain values of the target variable.\n",
    "\n",
    "You can imagine the analysis you are doing right now as a classification problem but you don't have any information about the affiliation to a class of any of the dataset entries. The best way to deal with this is to take an educated guess on the number of classes and the affiliation of every dataset to theses classes.\n",
    "\n",
    "One way to take that guess is by determining a number of classes `k` and choosing `k` random points in the dataset. The next step is to calculate the distances between every entry of the dataset and the random points and assigning each dataset entry to the random point it has the lowest sum of distances (one distance for every feature) to. The third step is updating the random `k` points by moving it to the point in the dataset where it has to lowest sum of distances to all the point that were assigned to it in the previous step. It is now the `clustering center` or `centroid` of these points. The second and thiord step are then repeated until the `centroids` aren't updated anymore or a number of maximal iterations is reached. \n",
    "\n",
    "A method to improve the initial guess of random points is to plot a clustering metrics' values to different possible values for `k` as the so called `elbow plot`. The x-axes shows the possible values for `k` and the y-axes shows the value of the clustering metric e.g. `Silhouette Score`. The best number for `k` is then determined by looking for the most obvious bend in the resulting line.\n",
    "\n",
    "The above mentioned metric of `Silhouette Score` or `Silhouette Coefficient` is a ratio of distance between the mean intra cluster distance and the distance between a datapoint and the nearest cluster it is not a part of. Most of the other clustering metrics use some kind of distance ratio in-between datapoints in a cluster and between clusters or their centroids to determine if one cluster result is better than the other.\n",
    "\n",
    "### Exercise:\n",
    "\n",
    "- Import `KMeans` from scikit-learn's `cluster` module.\n",
    "- Import `silhouette_score` and `calinski_harabasz_score` from `sklearn.metrics`.\n",
    "- Import `matplotlib.pyplot`.\n",
    "- Complete the loop to calculate the silhouette scores for k = [2,3,4,5,6] and save it to a list.\n",
    "- Call `matplotlib.pyplot`'s `.subplots()` function and set the figsize width to `16` inches and the height to `10` inches. Give the `title`,`xlabel` and `ylabel` fitting values.\n",
    "- Create the same elbow plot using the `calinski_harabasz_score`\n",
    "- Choose a `k` for fitting the k-means algorithmn on the data based on the implications of the elbow plots.\n",
    "- Fit the `KMeans` object with the chosen `k` on the reduced dataset.\n",
    "- Explain what ratio `calinksi_haberasz_score` calculates and how it differs from silhouette score.\n",
    "- Can you explain what the `max_iter` argument when initializing a k-means object does?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "___________________\n",
    "___________________\n",
    "___________________\n",
    "\n",
    "#Initialize k-means clustering object\n",
    "kmeans = KMeans()\n",
    "\n",
    "#Elbow Plot for k in kmeans with silhouette score\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(_____):\n",
    "    km = KMeans(n_clusters=k, \n",
    "                max_iter=300, \n",
    "                tol=1e-04, \n",
    "                init='k-means++', \n",
    "                n_init=10, \n",
    "                random_state=42, \n",
    "                algorithm='lloyd')\n",
    "    km.fit(data_reduced)\n",
    "    _______________.append(silhouette_score(data, km.labels_))\n",
    "\n",
    "#Show Plot and Set labels\n",
    "fig, ax = plt.subplots(____________)\n",
    "ax.plot(range(2, 7), silhouette_scores, 'bx-')\n",
    "ax.set_title(_______________)\n",
    "ax.set_xlabel(_____________)\n",
    "ax.set_ylabel(_______________)\n",
    "plt.xticks(range(2, 7))\n",
    "plt.tight_layout()\n",
    "_____________\n",
    "\n",
    "#Elbow Plot for k in kmeans with calinski haberasz score\n",
    "ch_scores = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Show Plot and Set labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Choose k for K-Means algorithmn\n",
    "km = KMeans(n_clusters=____, \n",
    "                max_iter=300, \n",
    "                tol=1e-04, \n",
    "                init='k-means++', \n",
    "                n_init=10, \n",
    "                random_state=42, \n",
    "                algorithm='lloyd')\n",
    "\n",
    "#Fit k-means model\n",
    "km_fitted = km.fit(__________)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empty Markdown for your answers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Clustering Results\n",
    "\n",
    "Other than calculating clustering metrics there is only one other way to calculate the quality of the results. That is visualizing clustering results and checking if the result seem to be following a natural pattern in the data.\n",
    "\n",
    "Of course one of the main problems of this is data with more than three dimensions is that it can not be visualized in a conventional python plot. The answer is of course dimensionality reduction. For the analysis you are doing right now it is most practical to use `PCA` because that is the only dimensionality reduction technique you know at this point.\n",
    "\n",
    "Note that plain old `PCA`, although being efficient and well justified theoratically, is not considered the best method to visualize high dimensional data. When interpreting visualizations of principal components you need to remember it only considers linear dependencies. `PCA` transforms the data into perpendicular principal components which can, especially when only visualizing two components in a plot, cause major distortions of the real relationships in the dataset. Some of the more sophisticated visualization methods that can visualize even nonlinear relationships of highdimensional data are `Kernel PCA` or `t-sne`.\n",
    "\n",
    "For now let's settle on visualizign the first two principal components colored by the cluster they were assigned to by `KMeans`.\n",
    "\n",
    "### Exercise:\n",
    "- Import `seaborn` and `numpy`.\n",
    "- Initialize a `PCA` object that reduces data to only two principal components.\n",
    "- Fit and transform the review data utilizing the ´PCA´ objects' methods.\n",
    "- Create a `pd.DataFrame` from the reduced dataset and give the columns fitting names.\n",
    "- Concatenate the clustering result to the DataFrame utilizing the `KMeans` objects' `.labels_` attribute. Give the new column a fitting name.\n",
    "- Give the scatterplot a fitting title and set its width to `16` and its height to `10`.\n",
    "- Create and display a `seaborn.scatterplot` by passing the right column names to the `x`,`y` and `hue` arguments. Also pass the DataFrame you just created to the `data` argument.\n",
    "- Complete the loop to print for random reviews in every cluster. Pass the length of the dataset to `numpy`'s `random.choice` function.\n",
    "- Complete the code to create a barplot of the amount of words in every cluster.\n",
    "- Interpret the plots and evaluate results. Did you get a good clustering result? Can you see any promising patterns? Either way give reasons for your answer.\n",
    "- Can you think of a kind of analysis that might have been more reasonable regarding the fact that the analysed dataset consists only of blobs of text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#Reduce dimensionality for plotting the clustering result\n",
    "pca = _____(n_components=____,___________)\n",
    "pca.fit(data)\n",
    "reduced_data = ____________\n",
    "\n",
    "#Create a visualization DataFrame of the two principal components and the clustering results\n",
    "visualizing_data = pd.DataFrame(_____________, \n",
    "                                columns=[__________________,_________________])\n",
    "\n",
    "target = pd.DataFrame(km_fitted.labels_,columns=[____________])\n",
    "visualizing_data = pd.concat([visualizing_data,target],axis=1)\n",
    "\n",
    "#Create Scatterplot of first two principal components colored by clustering results\n",
    "plt.figure(_______________)\n",
    "plt.title(________________)\n",
    "sns.scatterplot(x=________________,y=______________________,hue=____________,data=_______________)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#Get unique values for clustering results\n",
    "clusters = _______________________\n",
    "\n",
    "#Print four random examples of each cluster\n",
    "for i in clusters:\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    while counter <= ______:\n",
    "        \n",
    "        #Choose random index to get samples of clustering result\n",
    "        idx = np.random.choice(len(_______),1)\n",
    "\n",
    "        #Get value on random location\n",
    "        v = __________[__________].iloc[idx].values[0]\n",
    "\n",
    "        while v != i:\n",
    "\n",
    "            #Get new index until value at target[idx] is i\n",
    "            idx = ______________\n",
    "            v = _________________\n",
    "\n",
    "        #Print random instance\n",
    "        print('Data of sample: ', df.iloc[idx][________].values[0])\n",
    "        print('Cluster of sample: ', i)\n",
    "\n",
    "        counter ________\n",
    "    \n",
    "    print('')\n",
    "\n",
    "#Create Barplot of sum of words of different clusters\n",
    "data_with_clusters = pd.concat([data,target],axis=1)\n",
    "colors=['green','blue','orange','yellow','red']\n",
    "\n",
    "for i in clusters:\n",
    "\n",
    "    #Subset data of only one cluster\n",
    "    temp_df = data_with_clusters.loc[data_with_clusters['cluster'] == i]\n",
    "    temp_df = temp_df.drop(['cluster'],axis=1)\n",
    "    sum_of_features = temp_df.sum()\n",
    "\n",
    "    #Create Plot\n",
    "    sum_of_features.plot.bar(figsize=(16,10),color=colors[i],alpha=0.5)\n",
    "\n",
    "#Set title and layout\n",
    "title = _______________\n",
    "plt.title(title)\n",
    "plt.xlabel(___________________)\n",
    "plt.ylabel(___________________)\n",
    "plt.tight_layout()\n",
    "__________________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empty Markdown for your answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "business_intelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:23:19) [MSC v.1916 32 bit (Intel)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a62774195cc102c0c21d08b57bae8b19c53ad683a0ef6d500d2a8ab23822340e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
