{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b9b7376-e1b3-471b-929b-8cd9f6ac0785",
   "metadata": {},
   "source": [
    "# Regression with The California Housing Dataset\n",
    "In the previous chapter you learned about the classification aspect of supervised learning. In the following exercise, the target variable is the median house value for California block groups,\n",
    "expressed in hundreds of thousands of dollars. The dataset is called [California Housing Dataset](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html). Because this target value is continuous, *classification* algorithmns like KNN-Classifier aren't able to perform the task. The concept needed to solve this is called *regression*.\n",
    "\n",
    "Like MNIST, the California Housing Dataset is included in scikit-learn's datasets. Each sample represents a block group which is the smallest geographical unit for which the U.S.\n",
    "Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). The dataset contains aggregate data like the average number of rooms per household or the median income for every block group. The california housing data can directly be transformed into a dataframe using `as_frame = true` as an argument when calling `fetch_california_housing`.\n",
    "\n",
    "To get to know the California Housing Dataset a little better, let's look at the description scikit-learn provides and the types of data contained in california housing. It is also important that a dataset does not contain any `NaN` or `null` values if you want to apply any of the methods provided by scikit-learn. This can be checked with the `.info()` function of a Dataframe. It counts `null` or non-null values of each column.\n",
    "\n",
    "### Exercise:\n",
    "- Import `datasets` from `sklearn`, `matplotlib.pyplot` and `numpy`.\n",
    "- Load the california housing dataset using `.fetch_california_housing()` on `datasets` setting the `as_frame` argument to `True`.\n",
    "- Print the `.DESCR` of california housing.\n",
    "- Display `.head()` and `.info()` on the `.frame` attribute of california housing.\n",
    "- Plot the histogramm of `MedHouseVal` using the `.target` of california housing and `plt.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0edf1a9a-2ac5-49b4-a5cc-79336d627e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mbirkefeld\\git\\business_intelligence\\.venv\\Lib\\site-packages\\sklearn\\datasets\\_base.py:1518: UserWarning: Retry downloading from url: https://ndownloader.figshare.com/files/5976036\n",
      "  warnings.warn(f\"Retry downloading from url: {remote.url}\")\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 403: Forbidden",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load the california housing data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m california_housing = \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch_california_housing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mas_frame\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Print the DESCR of the dataset\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(california_housing.DESCR)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mbirkefeld\\git\\business_intelligence\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mbirkefeld\\git\\business_intelligence\\.venv\\Lib\\site-packages\\sklearn\\datasets\\_california_housing.py:177\u001b[39m, in \u001b[36mfetch_california_housing\u001b[39m\u001b[34m(data_home, download_if_missing, return_X_y, as_frame, n_retries, delay)\u001b[39m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mData not found and `download_if_missing` is False\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m logger.info(\n\u001b[32m    174\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDownloading Cal. housing from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(ARCHIVE.url, data_home)\n\u001b[32m    175\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m archive_path = \u001b[43m_fetch_remote\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m    \u001b[49m\u001b[43mARCHIVE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdirname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_home\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tarfile.open(mode=\u001b[33m\"\u001b[39m\u001b[33mr:gz\u001b[39m\u001b[33m\"\u001b[39m, name=archive_path) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    185\u001b[39m     cal_housing = np.loadtxt(\n\u001b[32m    186\u001b[39m         f.extractfile(\u001b[33m\"\u001b[39m\u001b[33mCaliforniaHousing/cal_housing.data\u001b[39m\u001b[33m\"\u001b[39m), delimiter=\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    187\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mbirkefeld\\git\\business_intelligence\\.venv\\Lib\\site-packages\\sklearn\\datasets\\_base.py:1512\u001b[39m, in \u001b[36m_fetch_remote\u001b[39m\u001b[34m(remote, dirname, n_retries, delay)\u001b[39m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m   1511\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1512\u001b[39m         \u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremote\u001b[49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1513\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (URLError, \u001b[38;5;167;01mTimeoutError\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py:214\u001b[39m, in \u001b[36murlretrieve\u001b[39m\u001b[34m(url, filename, reporthook, data)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03mRetrieve a URL into a temporary location on disk.\u001b[39;00m\n\u001b[32m    199\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    210\u001b[39m \u001b[33;03mdata file as well as the resulting HTTPMessage object.\u001b[39;00m\n\u001b[32m    211\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    212\u001b[39m url_type, path = _splittype(url)\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m contextlib.closing(\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m    215\u001b[39m     headers = fp.info()\n\u001b[32m    217\u001b[39m     \u001b[38;5;66;03m# Just return the local path and the \"headers\" for file://\u001b[39;00m\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# URLs. No sense in performing a copy unless requested.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py:189\u001b[39m, in \u001b[36murlopen\u001b[39m\u001b[34m(url, data, timeout, context)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    188\u001b[39m     opener = _opener\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py:495\u001b[39m, in \u001b[36mOpenerDirector.open\u001b[39m\u001b[34m(self, fullurl, data, timeout)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.process_response.get(protocol, []):\n\u001b[32m    494\u001b[39m     meth = \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     response = \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py:604\u001b[39m, in \u001b[36mHTTPErrorProcessor.http_response\u001b[39m\u001b[34m(self, request, response)\u001b[39m\n\u001b[32m    601\u001b[39m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[32m200\u001b[39m <= code < \u001b[32m300\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparent\u001b[49m\u001b[43m.\u001b[49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py:533\u001b[39m, in \u001b[36mOpenerDirector.error\u001b[39m\u001b[34m(self, proto, *args)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[32m    532\u001b[39m     args = (\u001b[38;5;28mdict\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhttp_error_default\u001b[39m\u001b[33m'\u001b[39m) + orig_args\n\u001b[32m--> \u001b[39m\u001b[32m533\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py:466\u001b[39m, in \u001b[36mOpenerDirector._call_chain\u001b[39m\u001b[34m(self, chain, kind, meth_name, *args)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    465\u001b[39m     func = \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\urllib\\request.py:613\u001b[39m, in \u001b[36mHTTPDefaultErrorHandler.http_error_default\u001b[39m\u001b[34m(self, req, fp, code, msg, hdrs)\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "\u001b[31mHTTPError\u001b[39m: HTTP Error 403: Forbidden"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the california housing data\n",
    "#california_housing = datasets.fetch_california_housing(as_frame=True)\n",
    "\n",
    "# Load the california housing data from CSV files\n",
    "features = pd.read_csv('features.csv', index_col=0)\n",
    "target = pd.read_csv('target.csv', index_col=0)\n",
    "\n",
    "# Create a combined dataframe\n",
    "california_housing_frame = pd.concat([features, target], axis=1)\n",
    "\n",
    "# Print the DESCR of the dataset\n",
    "print(california_housing.DESCR)\n",
    "\n",
    "print(type(california_housing))\n",
    "\n",
    "# Display the head and info of the Dataframe\n",
    "display(california_housing.frame.head())\n",
    "display(california_housing.frame.info())\n",
    "\n",
    "# Plot the histogram of the target\n",
    "n, bins, patches = plt.hist(x=california_housing.target, bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('MedHouseVal (100.000 $)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of the target')\n",
    "\n",
    "# Set a clean upper y-axis limit.\n",
    "maxfreq = n.max()\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)\n",
    "\n",
    "#Display the Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7043cd6-e96d-498f-acda-c2e3eda815bc",
   "metadata": {},
   "source": [
    "# Correlation Matrix, Heatmap and Feature Selection\n",
    "Since all features are numeric (`float64`) and there are no missing values in the dataset, you can continue the analysis without transforming the data. To identify whether there are any noticeable linear relationships in a dataset, it is advisable to compute the correlation matrix of the data. It will not only show the correlation between variables and target but also the correlation between the variables which can be valueable additional information.\n",
    "\n",
    "Seaborn offers a nice way to visualize the correlation matrix of a dataframe with its `heatmap()` function. First, you need to compute the correlation matrix of the data by using `corr()` on the dataframe before you can plot the values with the help of seaborn and matplotlib's `pyplot`. \n",
    "\n",
    "The most important part of the heatmap is the row/column that shows the relationship to the explanatory variables to the target `MedHouseVal`. The most important features for a *linear prediction model* (that can predict the target variable from the other explanatory variables) such as linear regression are the features with highest absolute correlation coefficents. It's also important to think about whether a high correlation between features or features and target variable makes sense or not based on your knowledge about the data. To take a closer look at the features with a high absolute correlation you can plot each combination with the target feature. If the datapoints in those plots already seem like they have structure  (e.g. by building clusters or having a proportional relationship), they might be well-suited for prediction.\n",
    "\n",
    "### Exercise:\n",
    "- Import `seaborn`.\n",
    "- Compute the correlation matrix using `.corr()` on the `.frame` attribute of california housing and display it.\n",
    "- Plot the correlation matrix using seaborn's `.heatmap()` method and make it visible.\n",
    "- Subset `MedInc` from the `.frame` attribute of the data and save it as an `np.array` to `x_inc` and `reshape(-1,1)` it.\n",
    "- Save the `.target` attribute of california housing to a `np.array` called `y` and `reshape(-1,1)` it.\n",
    "- Plot the Median House Value vs the Median income of a block group using pyplot's `.scatter()` method and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151de28-c5c6-41e7-a8cd-a7577b2ade25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "import seaborn as sns\n",
    "\n",
    "#Compute the correlation matrix and display it\n",
    "correlations = california_housing_frame.corr()\n",
    "display(correlations)\n",
    "\n",
    "#Plot the correlation matrix\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.title('Correlation Matrix of California Housing Data')\n",
    "sns.heatmap(correlations, annot=True)\n",
    "plt.show()\n",
    "\n",
    "#Subset MedInc\n",
    "x_inc = np.array(california_housing_frame['MedInc'])\n",
    "x_inc = x_inc.reshape(-1,1)\n",
    "\n",
    "#Create a variable y and save the target\n",
    "y = np.array(california_housing_frame['MedHouseVal'])\n",
    "\n",
    "#reshaping target and x_inc is necessary for fitting a simple linear regression model with scikit learn\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "#Create a Scatterplot of y and 'MedInc`\n",
    "plt.scatter(x_inc,y)\n",
    "plt.title('House Value vs. Income')\n",
    "plt.ylabel('Median House Value (100.000 $)')\n",
    "plt.xlabel('Median Income (1000 $)')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af830165-6d7e-4de5-8052-a23a36428dfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Linear Regression Models\n",
    "\n",
    "In statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\n",
    "\n",
    "In other words, a linear regression model fits a line to the data using the relationship between the features of dataset and its target. This line is represented by a function, e.g. $y = ax + b$ for simple linear regression with only one feature $x$ and parameters $a$ and $b$ to predict the target $y$. Choosing more than one feature to compute a linear regression model typically increases prediction accuracy and adds dimensions to the function: $y = a_1 * x_1 + a_2 * x_2 + ... + a_n * x_n + b$.\n",
    "\n",
    "The parameters of a linear model $a_i$ and $b$ are estimated from the training data. The commonly used method in linear regression modelling is the least-squares estimation. It minimizes the sum of the squares of the differences between the values of the dependent variable being observed in the given training dataset and those predicted by the linear function of the independent variable(s). Geometrically, this is seen as the sum of the squared distances, parallel to the axis of the dependent variable, between each data point in the set and the corresponding point on the regression surface — the smaller the differences, the better the model fits the data.\n",
    "\n",
    "Scikit-learn provides a number of objects in it's `linear_model` section. The `.fit()` method of those objects can be used to calculate the parameters of a regression function for any dataset . The `.predict()` function is used to predict the target values for e.g. test datasets using the fitted linear regression model. The mean squared error (MSE) of a linear regression model can be computed by inserting the actual target values ($y_i$) and the predictions of the fitted model ($ŷ_{i}$) in the following formula:\n",
    "\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - ŷ_{i})^{2}$$\n",
    "\n",
    "Luckily, `scikit-learn.metrics` provides a method to calculate the MSE called `mean_squared_error`. You can use it by passing the actual target values of a training or test dataset and their predicted counterparts. The best possible value of the MSE is `0`. It means that all predicted values are equal to the actual target values. The higher the MSE, the lower the accuracy of the predicted values. This can be very helpful in choosing the best regression model for a given dataset. You can use different subsets of explanatory variables to predict a datasets target variable with linear regression. After fitting a linear regression to every subset of explanatory variables, you predict the target values with every model. Then you compute the MSE for every prediction and choose the linear regression model with the lowest mean squared error as it fits the data best. Note, that the best model is not always the model with the most explanatory variables. If you want to compare different regression models to find out which one is the best for a given dataset you need to make sure you fit on the exact same training data and predict on the exact same test data, otherwise the results and their respective MSEs are not comparable.\n",
    "\n",
    "In the following exercise you will first create a simple linear regression model using the median income since it has the highest correlation coefficient (`.69`). After that you will compute a model with two additional features and compare the mean squared errors of both models on the test set.\n",
    "\n",
    "### Exercise:\n",
    "- Import `pandas`, `LinearRegression` from `sklearn.linear_model`, `train_test_split` from `sklearn.model_selection` and `mean_squared_error` from `sklearn.metrics`.\n",
    "- Perform `train_test_split` on `y` and `x_inc`. Use `0.2` for the size of the test set and `42` as the random state.\n",
    "- Create a model `simple_model` using the `LinearRegression()` method.\n",
    "- Fit simple linear regression model `simple_model` using `.fit()` on `X_train` and `y_train`.\n",
    "- Compute predictions for the median house value in the test set calling the model's `.fit()` method passing `X_test` as an argument and store the predictions in `y_pred`.\n",
    "- Compute the mean squared error of the simple linear model by calling `mean_squared_error()` on `y_test` and `y_pred`, store it in `simple_model_mse` and print it.\n",
    "- Plot the resulting regression line using numpy's `.linspace()` and the `.predict()` method of `simple_model` in a scatterplot.\n",
    "- Subset the features `MedInc`, `HouseAge` and `AveRooms` and save them into a pandas dataframe `X`.\n",
    "- Overwrite `X_train`, `X_test`. `y_train` and `y_test` using `train_test_split` on `y` and `X`. Test size and random state must remain the same as before to make the results comparable.\n",
    "- Create and fit a multiple linear regression model `multiple_model` on `X_train` and `y_train` to create a regression model with three predictors.\n",
    "- Compute predictions with the multiple linear model and store them in `y_pred`.\n",
    "- Compute the mean squared error of the multiple model and compare it to the score of the simple model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea81683-0978-481e-9ec5-c8bc61da3c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary modules\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "#Split x_inc and y into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_inc, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Create and fit a simple linear regression model\n",
    "simple_model = LinearRegression()\n",
    "simple_model.fit(X_train, y_train)\n",
    "\n",
    "#Compute predictions for X_test\n",
    "y_pred = simple_model.predict(X_test)\n",
    "\n",
    "#Compute the MSE of simple_model\n",
    "simple_model_mse = mean_squared_error(y_test, y_pred)\n",
    "print(simple_model_mse)\n",
    "\n",
    "#Plot the regression line of simple_model in a scatterplot\n",
    "prediction_space = np.linspace(min(x_inc),max(x_inc)).reshape(-1, 1)\n",
    "plt.scatter(X_train,y_train)\n",
    "plt.title('Regression Line of Simple Regression Model')\n",
    "plt.ylabel('Median House Value (100.000 $)')\n",
    "plt.xlabel('Median Income (1000 $)')\n",
    "plt.plot(prediction_space, simple_model.predict(prediction_space), color='black', linewidth=3)\n",
    "plt.show()\n",
    "\n",
    "#Subset features\n",
    "X = pd.DataFrame(california_housing_frame[['MedInc','HouseAge','AveRooms']])\n",
    "\n",
    "#Split x and y into train and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Create and fit a multiple regression model\n",
    "multiple_model = LinearRegression()\n",
    "multiple_model.fit(X_train, y_train)\n",
    "\n",
    "#Compute predictions with the multiple regression model\n",
    "y_pred = multiple_model.predict(X_test)\n",
    "\n",
    "#Compute the MSE of multiple_model\n",
    "multiple_model_mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "#Execute following code to find out which model has the lower mean squared error\n",
    "if (simple_model_mse > multiple_model_mse):\n",
    "    print('The multiple model has the lower MSE of: ', multiple_model_mse)\n",
    "else:\n",
    "    print('The simple model has the lower MSE of: ', simple_model_mse)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3562c34-934a-4209-936f-acf2b1db708c",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "As you may have noticed you needed to keep the `random_state` in `train_test_split` the same when wanting to compare the two different linear models. That is because the score of a prediction model is depending on the point of the split between train and test data. The perfomance on the test set may be biased because `train_test_split` randomly chose a particularly \"easy\" or \"hard\" to predict part of the data as test set. To avoid this the concept of *cross validation* can be used. Cross validation splits the data into $K$ folds. \n",
    "\n",
    "<img src='KfoldCV.gif'>\n",
    "\n",
    "*Hint*: For better visibility of the above picture: Switch Theme to JupyterLab Light if you are using JupyterLab Dark and switch back to dark mode after understanding the displayed process.\n",
    "\n",
    "Then, the process of training and testing is performed on these folds $K$ times with changing folds being the test set in every iteration until every fold was tested on once. Consequently, you have multiple score values that you need to average. The score of a cross validated linear model is more reliable than a score of a simple split validation. That means that it is more likely to be near the score it will actually have when confronted with new data. This is called the *generalization ability* of a prediction model and is one of the most important aspects when evaluating any model.  \n",
    "\n",
    "Scikit learn provides a *cross-validation* function called `cross_val_score` in its `.model_selection`. It can be used by passing a variable of scikit learn's `.linear_model` class, the data, the target and the number of folds you would like to do. In each fold the process of fitting the model on the training data and computing a score on the test set is performed with the only difference of changing test and training set in the way the above picture illustrates.\n",
    "\n",
    "In the following exercises you will compute a linear regression model using cross validation on the familiar California Housing Dataset. The mean squared error of the cross validation function of scikit-learn outputs negative values for optimization purposes which aren't contents of this course. It causes the necessity to multiply the output of `cross_val_score` by `-1` to make it interpretable which has already been done for you.\n",
    "\n",
    "### Exercise:\n",
    "- Import `cross_val_score` from `sklearn.model_selection` and `numpy`.\n",
    "- Create a `LinearRegression()` object.\n",
    "- Compute 5-fold cross validation MSEs by using `cross_val_score` passing the `LinearRegression()` object, the `.data` and the `.target` attributes of `california_housing`, the number of folds and `'neg_mean_squared_error'` as arguments.\n",
    "- Print the MSEs.\n",
    "- Compute and print the average 5-fold cross validation MSE of the model using numpy's `.mean()`.\n",
    "- Compute and print cross validation MSEs and average score of a 10-fold cross validation MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b0d1099-b319-4440-95e4-d4d1f01c22f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scores of 5-fold-cv:  [0.48485857 0.62249739 0.64621047 0.5431996  0.49468484]\n",
      "average score of 5-fold-cv:  0.5582901717686811\n",
      "scores of 10-fold-cv:  [0.48922052 0.43335865 0.8864377  0.39091641 0.7479731  0.52980278\n",
      " 0.28798456 0.77326441 0.64305557 0.3275106 ]\n",
      "average score of 10-fold-cv:  0.550952429695663\n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "#Create a linear regression object\n",
    "reg = LinearRegression()\n",
    "\n",
    "#Compute cross validation MSEs of 5-fold-cv\n",
    "neg_cv_scores5 = cross_val_score(reg, features.values, target.values, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_scores5 = neg_cv_scores5 * -1\n",
    "\n",
    "#print cross validation MSEs\n",
    "print(\"scores of 5-fold-cv: \",cv_scores5)\n",
    "\n",
    "#Compute mean cv MSE of 5-fold-cv\n",
    "mean_cv_score = np.mean(cv_scores5)\n",
    "print(\"average score of 5-fold-cv: \", mean_cv_score)\n",
    "\n",
    "#Compute and print 10-fold-cv MSEs and average MSEs\n",
    "neg_cv_scores10 = cross_val_score(reg, features.values, target.values, cv=10, scoring='neg_mean_squared_error')\n",
    "cv_scores10 = neg_cv_scores10 * -1                              \n",
    "print(\"scores of 10-fold-cv: \",cv_scores10)\n",
    "print(\"average score of 10-fold-cv: \", np.mean(cv_scores10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efd4dc3b",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "Now that you now about preprocessing data and learned to fit a machine learning model to clean data in the previous chapters, it's time to bring it all together. *Pipelines* are a way to combine all steps of data analysis in one place. Thereby, they increase the reusability and clarity of code because a Pipeline can be applied to similiar data science problems with minimal adjustment.\n",
    "\n",
    "As you might already have expected, `scikit-learn` provides an object to implement a machine learning `Pipeline`. In the following exercise you will bring it all together and build two machine learning pipelines using classification and regression algorithmns. You will also be cross validating them on the given data as you already did in exercise 6.\n",
    "\n",
    "Additionally, there is also a way to integrate the evaluation step into a `Pipeline`. For example, if you wanted to evaluate whether a simple linear model or a multiple linear model has a better cross validation score, that step can also be part of the `Pipeline` used for the regression task. \n",
    "\n",
    "Another aspect of this evaluation is the search for the best parameters of the algorithmn used to complete the task. An example of that may be the search for the best k in k-nearest neighbor classification which you applied in exercise 4 and 5. There are also different ways of computing the distance between the datapoints in k-nearest neighbor based algorithmns so might another way to fit the prediction model to the data on which it is applied on. This task of finding the perfect parameters for the chosen machine learning model is called `Hyperparameter Tuning` and can be implemented using scikit-learn's `GridSearchCV` method. This method takes a dictionary containing the possible hyperparameters for the given model and performs k-fold crossvalidation on every possible combination of parameters. It saves the hyperparameter configuration with the best mean cross validation score und then uses it when calling the `.predict()` method. However `GridSearchCV` is not the only way to optimize hyperparameters using cross validation because if the dataset is very large, the model is very complex or the hyperparameter space is very large it may take too much computional cost to compute every single hyperparameter combination and compare the results (full factorial search). If the problem to solve is taking too much time when using `GridSearchCV` it is a valid option to use `RandomSearchCV` which may not find the ultimate best hyperparameter combination but at least a good one in a possibly small fraction of the computation time of a full factorial search.\n",
    "\n",
    "What you also already might have noticed, is that scikit-learn operates on numpy arrays. That's why you needed to convert the result, a scikit-learn model calculated, back to a pandas DataFrame and save the column names before computing if you wanted to use pandas methods to explore the data and keep the good readability of panda's data description methods, although it's not necessary to do this. It is a matter of taste if you use pandas Dataframes or numpy's arrays and matrices to compute and explore your data. Both modules have their pros and cons and it's advisable to choose a module on what is best for the specific machine learning task.\n",
    "\n",
    "**Opinion**: Use pandas whenever possible.\n",
    "\n",
    "In the following exercises you will be creating a regression pipeline with the familiar k-nearest neighbor algorithmn. You will also use `GridSearchCV` to find the best hyperparameters for this specific regression task. The first few tasks will all be preparation steps, setting up the pipeline with scaling method and regression algorithmn before  setting up a hyperparameter space and creating a `GridSearchCV` object. All of the computations happen when calling the `.fit()` method of the `GridSearchCV` object meaning all of the preprocessing steps, fitting the model to the data and searching for the best hyperparameters.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "- Import `Pipeline` from `sklearn`'s `pipeline`, `StandardScaler` from `sklearn`'s `Preprocessing`, the `KNeighborsRegressor` from `sklearn`'s `neighbors` and the `GridSearchCV` object from `sklearn.model_selection`.\n",
    "- Create the regression pipeline by utilizing `Pipeline` and passing it `StandardScaler` naming the step `scaler` as well as passing `KNeighborsRegressor` naming the step `knn_regressor`. Make sure you name the pipeline's steps right because this will be important when searching for the best hyperparameters.\n",
    "- Create a dictionary containing the hyperparameters named `params`. Make sure to type the key in the form `model__hyperparameter` and the values as list using square brackets.\n",
    "- The values for `n_neighbors` are three, five, seven and nine. The values for `weights` are `uniform` and `distance`. The values for `p` are one, two and three.\n",
    "- Do you know what the `p` stands for when using the k-nearest neigbor algorithmn ? Type your answer below. *Hint*: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\">KNN-Regressor Documentation</a>\n",
    "- Create a `GridSearchCV` object and pass the pipeline to the `estimator` argument and the hyperparameter grid to the `param_grid` argument.\n",
    "- Fit the `GridSearchCV` object to the training data.\n",
    "- Print the best hyperparameters using the `GridSearchCV` object's `best_paramas_` attribute.\n",
    "- Set `reg` to the best found estimator in grid search using the `GridSearchCV`'s `best_estimator_` attribute.\n",
    "- Make predictions on `X_test` utilizing the best estimator.\n",
    "- Calculate the mean squared error on the Testset und the 5 fold cross validation scores as well as their mean.\n",
    "- What is your interpretation of the score compared to what the linear regression model scored?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8545989",
   "metadata": {},
   "source": [
    "# Type first text based answer here: What does p do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4834bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Create a regression Pipeline\n",
    "knn_pipe = ________([('scaler',______________()),('_____________',___________________())])\n",
    "\n",
    "#Create a dictionary for hyperparameters\n",
    "params = {'_______________n_neighbors': [__________],\n",
    "                '______________________': ['uniform','________'],\n",
    "                '________________':[1,2,3]}\n",
    "\n",
    "#Create grid search object\n",
    "grid_search = ____________(estimator=________,__________=______)\n",
    "\n",
    "#Calculate best model for given task by fitting the grid search object to the training data\n",
    "___________.___(_______, _______)\n",
    "\n",
    "#Print best parameters\n",
    "print(___________.____________)\n",
    "\n",
    "#Create best estimator from grid search results\n",
    "reg = __________________________\n",
    "\n",
    "#Predict Test set\n",
    "y_pred_pipe = ___._______(______)\n",
    "\n",
    "#Print mean squared error\n",
    "_____(mean_squared_error(______,___________))\n",
    "\n",
    "#Compute cross validation MSEs of 5-fold-cv\n",
    "neg_cv_scores5 = _______________(___, california_housing.____, california_housing.______, cv=_, scoring='neg_mean_squared_error')\n",
    "cv_scores5 = neg_cv_scores5 * -1\n",
    "\n",
    "#Print cross validation MSEs\n",
    "print(\"scores of 5-fold-cv: \",__________)\n",
    "\n",
    "#Compute mean cv MSE of 5-fold-cv\n",
    "mean_cv_score = __.____(__________)\n",
    "print(\"average score of 5-fold-cv: \", ____________)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ceac5b5d",
   "metadata": {},
   "source": [
    "# Type second text based answer here: Interpretation of scores and comparison to scores of linear models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
